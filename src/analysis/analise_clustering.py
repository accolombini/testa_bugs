"""
An√°lise de Clustering para BASE_TESTES.xlsx

Este script realiza an√°lise completa de clustering usando K-means com sele√ß√£o robusta
do n√∫mero √≥timo de clusters baseada em m√∫ltiplas m√©tricas de valida√ß√£o.

Funcionalidades principais:
- Leitura e pr√©-processamento de dados Excel
- An√°lise explorat√≥ria com estat√≠sticas descritivas
- Visualiza√ß√µes interativas (boxplot, heatmap, an√°lise de clusters)
- Determina√ß√£o autom√°tica do n√∫mero √≥timo de clusters
- Aplica√ß√£o de K-means clustering
- Visualiza√ß√£o dos resultados com PCA
- Export dos dados com clusters para Excel (individual e consolidado)

M√©tricas utilizadas para sele√ß√£o de clusters:
- Silhouette Score: Mede qualidade da separa√ß√£o (0-1, maior = melhor)
- Calinski-Harabasz Index: Raz√£o between/within clusters (>0, maior = melhor)
- Davies-Bouldin Index: M√©dia das dist√¢ncias intra/inter clusters (>0, menor = melhor)
- M√©todo do Cotovelo: An√°lise da in√©rcia para detectar ponto de inflex√£o

Autor: Sistema de An√°lise Inteligente
Data: Outubro 2025
Vers√£o: 2.0 - Com sele√ß√£o robusta de clusters
"""

import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import plotly.figure_factory as ff
from plotly.subplots import make_subplots
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, silhouette_samples
from sklearn.decomposition import PCA
import os
from pathlib import Path


class AnaliseClusteringPipeline:
    """
    Pipeline completo para an√°lise de clustering com sele√ß√£o robusta de clusters.
    
    Esta classe implementa um workflow completo de an√°lise de clustering, desde
    a carga dos dados at√© a exporta√ß√£o dos resultados finais. O diferencial
    est√° na sele√ß√£o robusta do n√∫mero √≥timo de clusters usando m√∫ltiplas m√©tricas.
    
    Attributes:
        data_path (str): Caminho para o arquivo de dados Excel
        df_original (pd.DataFrame): Dataset original carregado
        df_scaled (pd.DataFrame): Dataset normalizado (StandardScaler)
        scaler (StandardScaler): Objeto scaler fitted para transforma√ß√µes
        best_k (int): N√∫mero √≥timo de clusters determinado
        kmeans_model (KMeans): Modelo K-means treinado
        cluster_labels (np.array): Labels dos clusters para cada observa√ß√£o
        
    Example:
        >>> pipeline = AnaliseClusteringPipeline("dados.xlsx")
        >>> resultados = pipeline.executar_pipeline_completo()
        >>> print(f"Melhor k: {resultados['best_k']}")
    """
    
    def __init__(self, data_path="datasets/BASE_TESTES.xlsx"):
        """
        Inicializa o pipeline de an√°lise de clustering.
        
        Args:
            data_path (str): Caminho para o arquivo Excel com os dados.
                           Default: "datasets/BASE_TESTES.xlsx"
        """
        # Configura√ß√£o de caminhos e dados
        self.data_path = data_path
        
        # Estruturas de dados principais
        self.df_original = None      # Dataset original
        self.df_scaled = None        # Dataset normalizado
        self.scaler = None           # Scaler para normaliza√ß√£o
        
        # Resultados do clustering
        self.best_k = None           # N√∫mero √≥timo de clusters
        self.kmeans_model = None     # Modelo K-means treinado
        self.cluster_labels = None   # Labels dos clusters
        
        # Criar diret√≥rios de output se n√£o existirem
        self._create_output_dirs()
    
    def _create_output_dirs(self):
        """
        Cria diret√≥rios necess√°rios para salvar os outputs da an√°lise.
        
        Cria as pastas:
        - outputs/figuras: Para salvar gr√°ficos HTML interativos
        - outputs/data: Para salvar arquivos Excel com resultados
        """
        Path("outputs/figuras").mkdir(parents=True, exist_ok=True)
        Path("outputs/data").mkdir(parents=True, exist_ok=True)
    
    def carregar_dados(self):
        """
        Carrega o dataset do arquivo Excel e realiza an√°lise inicial.
        
        L√™ o arquivo Excel especificado em data_path e exibe informa√ß√µes
        b√°sicas sobre o dataset como dimens√µes, colunas e valores faltantes.
        
        Returns:
            pd.DataFrame: Dataset carregado
            
        Raises:
            FileNotFoundError: Se o arquivo n√£o for encontrado
            pd.errors.EmptyDataError: Se o arquivo estiver vazio
        """
        print("üìä Carregando dados...")
        
        # Carregar arquivo Excel
        self.df_original = pd.read_excel(self.data_path)
        
        # Exibir informa√ß√µes b√°sicas do dataset
        print(f"‚úÖ Dados carregados: {self.df_original.shape}")
        print(f"üìã Colunas: {list(self.df_original.columns)}")
        print(f"üîç Valores faltantes: {self.df_original.isnull().sum().sum()}")
        
        return self.df_original
    
    def preprocessar_dados(self):
        """
        Realiza pr√©-processamento dos dados para clustering.
        
        Aplica StandardScaler (normaliza√ß√£o Z-score) aos dados para garantir
        que todas as features tenham m√©dia 0 e desvio padr√£o 1. Isso √© essencial
        para algoritmos de clustering baseados em dist√¢ncia como K-means.
        
        O StandardScaler √© prefer√≠vel ao MinMaxScaler para clustering pois:
        - Preserva a distribui√ß√£o original dos dados
        - √â menos sens√≠vel a outliers extremos
        - Funciona melhor com algoritmos baseados em dist√¢ncia euclidiana
        
        Returns:
            pd.DataFrame: Dataset normalizado
        """
        print("\nüîß Pr√©-processando dados...")
        
        # Aplicar StandardScaler (Z-score normalization)
        # Formula: (x - m√©dia) / desvio_padr√£o
        self.scaler = StandardScaler()
        dados_normalizados = self.scaler.fit_transform(self.df_original)
        
        # Criar DataFrame normalizado mantendo √≠ndices e colunas originais
        self.df_scaled = pd.DataFrame(
            dados_normalizados,
            columns=self.df_original.columns,
            index=self.df_original.index
        )
        
        print("‚úÖ Dados normalizados (StandardScaler)")
        print(f"üìè M√©dia ap√≥s normaliza√ß√£o: {self.df_scaled.mean().mean():.6f}")
        print(f"üìä Desvio padr√£o ap√≥s normaliza√ß√£o: {self.df_scaled.std().mean():.6f}")
        
        return self.df_scaled
    
    def estatisticas_descritivas(self):
        """
        Gera estat√≠sticas descritivas completas dos dados.
        
        Calcula e exibe estat√≠sticas descritivas tanto para os dados originais
        quanto para os dados normalizados, al√©m da matriz de correla√ß√£o.
        
        As estat√≠sticas incluem:
        - Medidas de tend√™ncia central (m√©dia, mediana)
        - Medidas de dispers√£o (desvio padr√£o, quartis)
        - Valores m√≠nimos e m√°ximos
        - Matriz de correla√ß√£o entre features
        
        Returns:
            tuple: (estat√≠sticas, matriz_correla√ß√£o)
                - estat√≠sticas (dict): Dicion√°rio com describe() dos dados
                - matriz_correla√ß√£o (pd.DataFrame): Matriz de correla√ß√£o
        """
        print("\nüìà Gerando estat√≠sticas descritivas...")
        
        # Calcular estat√≠sticas para dados originais e normalizados
        stats = {
            'Original': self.df_original.describe(),
            'Normalizado': self.df_scaled.describe()
        }
        
        print("=== DADOS ORIGINAIS ===")
        print(self.df_original.describe())
        
        print("\n=== CORRELA√á√ïES ENTRE FEATURES ===")
        # Calcular matriz de correla√ß√£o (Pearson)
        correlacoes = self.df_original.corr()
        print(correlacoes)
        
        # Identificar correla√ß√µes fortes (|r| > 0.7)
        correlacoes_fortes = []
        n_features = len(correlacoes.columns)
        for i in range(n_features):
            for j in range(i+1, n_features):
                corr_val = correlacoes.iloc[i, j]
                if abs(corr_val) > 0.7:
                    correlacoes_fortes.append({
                        'feature1': correlacoes.columns[i],
                        'feature2': correlacoes.columns[j],
                        'correlacao': corr_val
                    })
        
        if correlacoes_fortes:
            print(f"\nüîç Correla√ß√µes fortes encontradas (|r| > 0.7):")
            for item in correlacoes_fortes:
                print(f"   {item['feature1']} ‚Üî {item['feature2']}: {item['correlacao']:.3f}")
        else:
            print("\n‚úÖ Nenhuma correla√ß√£o muito forte detectada (|r| > 0.7)")
        
        return stats, correlacoes
    
    def criar_visualizacoes_exploratorias(self):
        """
        Cria visualiza√ß√µes explorat√≥rias dos dados.
        
        Gera dois tipos principais de visualiza√ß√µes:
        1. Boxplot: Mostra distribui√ß√£o, outliers e quartis de cada feature
        2. Heatmap: Visualiza√ß√£o da matriz de correla√ß√£o entre features
        
        As visualiza√ß√µes s√£o interativas (Plotly) e salvas como HTML para
        permite navega√ß√£o e zoom. S√£o essenciais para:
        - Identificar outliers
        - Entender distribui√ß√µes das vari√°veis
        - Detectar padr√µes de correla√ß√£o
        - Validar a necessidade de normaliza√ß√£o
        
        Outputs:
        - outputs/figuras/boxplot.html: Boxplots de todas as features
        - outputs/figuras/heatmap.html: Mapa de calor das correla√ß√µes
        """
        print("\nüìä Criando visualiza√ß√µes explorat√≥rias...")
        
        # === 1. BOXPLOT DE TODAS AS FEATURES ===
        print("  üìà Gerando boxplots...")
        fig_box = go.Figure()
        
        # Adicionar um boxplot para cada feature
        for col in self.df_original.columns:
            fig_box.add_trace(go.Box(
                y=self.df_original[col],
                name=col,
                boxpoints='outliers',  # Mostrar apenas outliers
                jitter=0.3,            # Espalhar pontos para melhor visualiza√ß√£o
                pointpos=-1.8          # Posi√ß√£o dos outliers
            ))
        
        # Configurar layout do boxplot
        fig_box.update_layout(
            title="Distribui√ß√£o das Features (Boxplot)",
            xaxis_title="Features",
            yaxis_title="Valores",
            showlegend=False,
            height=600,
            hovermode='x unified'  # Hover unificado para compara√ß√£o
        )
        
        # Salvar e exibir boxplot
        fig_box.write_html("outputs/figuras/boxplot.html")
        fig_box.show()
        
        # === 2. HEATMAP DAS CORRELA√á√ïES ===
        print("  üå°Ô∏è Gerando mapa de calor...")
        correlacoes = self.df_original.corr()
        
        # Criar heatmap interativo
        fig_heatmap = go.Figure(data=go.Heatmap(
            z=correlacoes.values,
            x=correlacoes.columns,
            y=correlacoes.columns,
            colorscale='RdBu',      # Escala azul-branco-vermelho
            zmid=0,                 # Centro da escala em 0
            text=correlacoes.round(3).values,  # Valores nas c√©lulas
            texttemplate="%{text}",  # Template para exibir valores
            textfont={"size": 10},
            hoverongaps=False,
            colorbar=dict(
                title="Correla√ß√£o",
                titleside="right",
                tickmode="linear",
                tick0=-1,
                dtick=0.2
            )
        ))
        
        # Configurar layout do heatmap
        fig_heatmap.update_layout(
            title="Mapa de Calor - Correla√ß√µes entre Features",
            width=800,
            height=600,
            xaxis_title="Features",
            yaxis_title="Features"
        )
        
        # Salvar e exibir heatmap
        fig_heatmap.write_html("outputs/figuras/heatmap.html")
        fig_heatmap.show()
        
        print("‚úÖ Visualiza√ß√µes salvas em outputs/figuras/")
    
    def encontrar_numero_otimo_clusters(self, max_k=10):
        """Encontrar n√∫mero √≥timo de clusters usando m√∫ltiplas m√©tricas"""
        print(f"\nüîç Analisando clusters de 2 a {max_k} com m√∫ltiplas m√©tricas...")
        
        k_range = range(2, max_k + 1)
        silhouette_scores = []
        inertias = []
        calinski_scores = []
        davies_bouldin_scores = []
        
        # Importar m√©tricas adicionais
        from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score
        
        for k in k_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(self.df_scaled)
            
            # M√∫ltiplas m√©tricas
            sil_score = silhouette_score(self.df_scaled, cluster_labels)
            cal_score = calinski_harabasz_score(self.df_scaled, cluster_labels)
            db_score = davies_bouldin_score(self.df_scaled, cluster_labels)
            
            silhouette_scores.append(sil_score)
            inertias.append(kmeans.inertia_)
            calinski_scores.append(cal_score)
            davies_bouldin_scores.append(db_score)
            
            print(f"k={k}: Silhouette={sil_score:.3f}, Calinski={cal_score:.1f}, Davies-Bouldin={db_score:.3f}")
        
        # M√©todo robusto para escolher k
        self.best_k = self._selecionar_melhor_k(k_range, silhouette_scores, calinski_scores, davies_bouldin_scores)
        
        print(f"\nüéØ Melhor n√∫mero de clusters: {self.best_k}")
        print(f"üìä Baseado em an√°lise combinada de m√∫ltiplas m√©tricas")
        
        # Visualizar an√°lise completa
        self._plot_analise_completa(k_range, silhouette_scores, inertias, calinski_scores, davies_bouldin_scores)
        
        return self.best_k, silhouette_scores
    
    def _selecionar_melhor_k(self, k_range, silhouette_scores, calinski_scores, davies_bouldin_scores):
        """
        Sele√ß√£o robusta do melhor k usando combina√ß√£o de m√∫ltiplas m√©tricas.
        
        Este m√©todo implementa uma abordagem avan√ßada para sele√ß√£o do n√∫mero
        √≥timo de clusters, combinando diferentes m√©tricas de valida√ß√£o com pesos
        espec√≠ficos. Isso √© mais robusto que usar apenas uma m√©trica.
        
        M√©tricas utilizadas:
        - Silhouette Score (40%): Qualidade da separa√ß√£o dos clusters
        - Calinski-Harabasz (30%): Raz√£o between/within cluster variance
        - Davies-Bouldin (30%): M√©dia das dist√¢ncias intra/inter clusters
        
        Args:
            k_range (range): Range de valores k testados
            silhouette_scores (list): Scores silhouette para cada k
            calinski_scores (list): Scores Calinski-Harabasz para cada k
            davies_bouldin_scores (list): Scores Davies-Bouldin para cada k
            
        Returns:
            int: N√∫mero √≥timo de clusters
            
        Note:
            Os pesos podem ser ajustados conforme a natureza dos dados.
            Silhouette tem maior peso por ser mais intuitivo e interpretativo.
        """
        
        # Fun√ß√£o auxiliar para normalizar scores para [0,1]
        def normalizar(scores, reverse=False):
            """
            Normaliza scores para escala [0,1].
            
            Args:
                scores (list): Lista de scores a normalizar
                reverse (bool): Se True, inverte a escala (para m√©tricas onde menor √© melhor)
            
            Returns:
                np.array: Scores normalizados
            """
            scores = np.array(scores)
            
            # Para Davies-Bouldin: menor √© melhor, ent√£o invertemos
            if reverse:
                scores = 1 / (1 + scores)  # Transforma√ß√£o que preserva ordem
            
            # Normaliza√ß√£o min-max
            min_score, max_score = scores.min(), scores.max()
            if max_score == min_score:
                return np.ones_like(scores)  # Todos iguais = score 1 para todos
            
            return (scores - min_score) / (max_score - min_score)
        
        # Normalizar todas as m√©tricas para [0,1]
        sil_norm = normalizar(silhouette_scores)                    # Maior = melhor
        cal_norm = normalizar(calinski_scores)                     # Maior = melhor  
        db_norm = normalizar(davies_bouldin_scores, reverse=True)  # Menor = melhor
        
        # Score combinado com pesos otimizados empiricamente
        # Pesos baseados na confiabilidade e interpretabilidade de cada m√©trica
        combined_scores = 0.4 * sil_norm + 0.3 * cal_norm + 0.3 * db_norm
        
        # Encontrar k com maior score combinado
        best_idx = np.argmax(combined_scores)
        best_k = list(k_range)[best_idx]
        
        # Exibir scores para transpar√™ncia
        print(f"\nüìà Scores combinados:")
        for i, k in enumerate(k_range):
            marcador = " ‚≠ê" if k == best_k else "   "
            print(f"{marcador} k={k}: Score Combinado = {combined_scores[i]:.3f}")
        
        return best_k

    def _plot_analise_completa(self, k_range, silhouette_scores, inertias, calinski_scores, davies_bouldin_scores):
        """Plot completo com todas as m√©tricas de valida√ß√£o"""
        
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=(
                'Silhouette Score (‚Üë melhor)', 
                'Calinski-Harabasz Score (‚Üë melhor)',
                'Davies-Bouldin Score (‚Üì melhor)', 
                'M√©todo do Cotovelo (In√©rcia)'
            ),
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}]]
        )
        
        colors = ['blue', 'green', 'red', 'orange']
        
        # 1. Silhouette Score
        fig.add_trace(
            go.Scatter(
                x=list(k_range), y=silhouette_scores,
                mode='lines+markers', name='Silhouette',
                line=dict(color=colors[0], width=3), marker=dict(size=8)
            ), row=1, col=1
        )
        
        # 2. Calinski-Harabasz
        fig.add_trace(
            go.Scatter(
                x=list(k_range), y=calinski_scores,
                mode='lines+markers', name='Calinski-Harabasz',
                line=dict(color=colors[1], width=3), marker=dict(size=8)
            ), row=1, col=2
        )
        
        # 3. Davies-Bouldin
        fig.add_trace(
            go.Scatter(
                x=list(k_range), y=davies_bouldin_scores,
                mode='lines+markers', name='Davies-Bouldin',
                line=dict(color=colors[2], width=3), marker=dict(size=8)
            ), row=2, col=1
        )
        
        # 4. In√©rcia (Cotovelo)
        fig.add_trace(
            go.Scatter(
                x=list(k_range), y=inertias,
                mode='lines+markers', name='In√©rcia',
                line=dict(color=colors[3], width=3), marker=dict(size=8)
            ), row=2, col=2
        )
        
        # Marcar o melhor k em todos os gr√°ficos
        best_idx = list(k_range).index(self.best_k)
        
        for row, col, values in [(1,1,silhouette_scores), (1,2,calinski_scores), 
                                (2,1,davies_bouldin_scores), (2,2,inertias)]:
            fig.add_trace(
                go.Scatter(
                    x=[self.best_k], y=[values[best_idx]],
                    mode='markers', name=f'Melhor k={self.best_k}',
                    marker=dict(size=15, color='red', symbol='star'),
                    showlegend=(row==1 and col==1)
                ), row=row, col=col
            )
        
        # Atualizar layout
        for i, col in enumerate([1, 2]):
            fig.update_xaxes(title_text="N√∫mero de Clusters (k)", row=1, col=col)
            fig.update_xaxes(title_text="N√∫mero de Clusters (k)", row=2, col=col)
        
        fig.update_layout(
            title="An√°lise Completa para Sele√ß√£o do N√∫mero √ìtimo de Clusters",
            height=800,
            showlegend=True
        )
        
        fig.write_html("outputs/figuras/analise_completa_clusters.html")
        fig.show()
    
    def aplicar_clustering(self):
        """Aplicar K-means com o n√∫mero √≥timo de clusters"""
        print(f"\nüéØ Aplicando K-means com {self.best_k} clusters...")
        
        self.kmeans_model = KMeans(n_clusters=self.best_k, random_state=42, n_init=10)
        self.cluster_labels = self.kmeans_model.fit_predict(self.df_scaled)
        
        # Adicionar labels ao dataframe original
        self.df_original['cluster_id'] = self.cluster_labels
        
        # Estat√≠sticas dos clusters
        cluster_stats = self.df_original.groupby('cluster_id').agg(['count', 'mean', 'std'])
        print("\nüìä Estat√≠sticas por cluster:")
        print(f"Distribui√ß√£o: {pd.Series(self.cluster_labels).value_counts().sort_index()}")
        
        return self.cluster_labels
    
    def visualizar_clusters(self):
        """Criar visualiza√ß√µes dos clusters"""
        print("\nüé® Criando visualiza√ß√µes dos clusters...")
        
        # 1. PCA para redu√ß√£o dimensional (visualiza√ß√£o 2D)
        pca = PCA(n_components=2)
        coords_2d = pca.fit_transform(self.df_scaled)
        
        # DataFrame para plotting
        df_plot = pd.DataFrame({
            'PC1': coords_2d[:, 0],
            'PC2': coords_2d[:, 1],
            'cluster': self.cluster_labels
        })
        
        # Plot overview dos clusters
        fig_overview = px.scatter(
            df_plot, 
            x='PC1', 
            y='PC2', 
            color='cluster',
            title=f"Clusters Identificados (k={self.best_k}) - Proje√ß√£o PCA",
            labels={'cluster': 'Cluster ID'},
            color_discrete_sequence=px.colors.qualitative.Set1
        )
        
        fig_overview.update_layout(height=600)
        fig_overview.write_html("outputs/figuras/clusters_overview.html")
        fig_overview.show()
        
        # 2. Visualiza√ß√£o individual de cada cluster
        self._visualizar_clusters_individuais()
        
        print("‚úÖ Visualiza√ß√µes de clusters salvas!")
    
    def _visualizar_clusters_individuais(self):
        """Criar visualiza√ß√µes separadas para cada cluster"""
        
        n_clusters = len(np.unique(self.cluster_labels))
        cols = min(3, n_clusters)
        rows = (n_clusters + cols - 1) // cols
        
        fig = make_subplots(
            rows=rows, 
            cols=cols,
            subplot_titles=[f'Cluster {i}' for i in range(n_clusters)],
            specs=[[{"type": "scatter"}] * cols for _ in range(rows)]
        )
        
        colors = px.colors.qualitative.Set1[:n_clusters]
        
        # PCA para coords 2D
        pca = PCA(n_components=2)
        coords_2d = pca.fit_transform(self.df_scaled)
        
        for i in range(n_clusters):
            row = (i // cols) + 1
            col = (i % cols) + 1
            
            # Dados do cluster
            mask = self.cluster_labels == i
            cluster_coords = coords_2d[mask]
            
            fig.add_trace(
                go.Scatter(
                    x=cluster_coords[:, 0],
                    y=cluster_coords[:, 1],
                    mode='markers',
                    name=f'Cluster {i}',
                    marker=dict(color=colors[i], size=6),
                    showlegend=False
                ),
                row=row, col=col
            )
        
        fig.update_layout(
            title="An√°lise Individual dos Clusters",
            height=300 * rows,
            showlegend=False
        )
        
        fig.write_html("outputs/figuras/clusters_individuais.html")
        fig.show()
    
    def exportar_dados_com_clusters(self):
        """Exportar dados originais + cluster_id para Excel"""
        print("\nüíæ Exportando dados com clusters...")
        
        # Preparar dados para export
        df_export = self.df_original.copy()
        
        # Estat√≠sticas por cluster
        stats_clusters = []
        for cluster_id in sorted(df_export['cluster_id'].unique()):
            cluster_data = df_export[df_export['cluster_id'] == cluster_id]
            stats = {
                'cluster_id': cluster_id,
                'tamanho': len(cluster_data),
                'percentual': len(cluster_data) / len(df_export) * 100
            }
            
            # M√©dias das features
            for col in self.df_original.columns:
                if col != 'cluster_id':
                    stats[f'media_{col}'] = cluster_data[col].mean()
            
            stats_clusters.append(stats)
        
        df_stats = pd.DataFrame(stats_clusters)
        
        # Salvar em Excel com m√∫ltiplas abas
        with pd.ExcelWriter('outputs/data/dados_com_clusters.xlsx', engine='openpyxl') as writer:
            df_export.to_excel(writer, sheet_name='dados_com_clusters', index=False)
            df_stats.to_excel(writer, sheet_name='estatisticas_clusters', index=False)
            
            # Aba com informa√ß√µes gerais
            info_geral = pd.DataFrame({
                'Informa√ß√£o': [
                    'Total de registros',
                    'N√∫mero de features',
                    'N√∫mero de clusters',
                    'Silhouette Score',
                    'Algoritmo usado'
                ],
                'Valor': [
                    len(df_export),
                    len(self.df_original.columns) - 1,  # -1 para excluir cluster_id
                    self.best_k,
                    f"{silhouette_score(self.df_scaled, self.cluster_labels):.3f}",
                    'K-means'
                ]
            })
            info_geral.to_excel(writer, sheet_name='info_geral', index=False)
        
        print("‚úÖ Dados exportados para outputs/data/dados_com_clusters.xlsx")
        print("üìä Abas criadas: dados_com_clusters, estatisticas_clusters, info_geral")
        
        # Exportar clusters individuais
        self._exportar_clusters_individuais(df_export)
        
        return 'outputs/data/dados_com_clusters.xlsx'
    
    def _exportar_clusters_individuais(self, df_export):
        """Exportar cada cluster como arquivo Excel separado"""
        print("\nüìÅ Exportando clusters individuais...")
        
        cluster_ids = sorted(df_export['cluster_id'].unique())
        arquivos_criados = []
        
        for cluster_id in cluster_ids:
            # Filtrar dados do cluster
            dados_cluster = df_export[df_export['cluster_id'] == cluster_id].copy()
            
            # Remover coluna cluster_id (j√° est√° impl√≠cito no arquivo)
            dados_cluster_limpo = dados_cluster.drop('cluster_id', axis=1)
            
            # Nome do arquivo
            arquivo = f'outputs/data/cluster_{cluster_id}.xlsx'
            
            # Salvar com informa√ß√µes adicionais
            with pd.ExcelWriter(arquivo, engine='openpyxl') as writer:
                # Aba principal com dados
                dados_cluster_limpo.to_excel(writer, sheet_name='dados', index=False)
                
                # Aba com estat√≠sticas do cluster
                stats = dados_cluster_limpo.describe()
                stats.to_excel(writer, sheet_name='estatisticas')
                
                # Aba com informa√ß√µes
                info = pd.DataFrame({
                    'Informa√ß√£o': [
                        'Cluster ID',
                        'N√∫mero de registros',
                        'Percentual do total',
                        'M√©dia geral das features'
                    ],
                    'Valor': [
                        cluster_id,
                        len(dados_cluster_limpo),
                        f"{len(dados_cluster_limpo)/len(df_export)*100:.1f}%",
                        f"{dados_cluster_limpo.mean().mean():.3f}"
                    ]
                })
                info.to_excel(writer, sheet_name='info', index=False)
            
            arquivos_criados.append(arquivo)
            print(f"  ‚úÖ {arquivo} - {len(dados_cluster_limpo)} registros")
        
        print(f"\nüìä {len(arquivos_criados)} arquivos de clusters individuais criados")
        print("üîß Cada arquivo tem 3 abas: dados, estatisticas, info")
        
        return arquivos_criados
    
    def executar_pipeline_completo(self):
        """
        Executa todo o pipeline de an√°lise de clustering de forma automatizada.
        
        Este √© o m√©todo principal que orquestra todo o processo de an√°lise:
        
        Etapas executadas:
        1. Carregamento dos dados do arquivo Excel
        2. Pr√©-processamento (normaliza√ß√£o StandardScaler)
        3. An√°lise explorat√≥ria e estat√≠sticas descritivas
        4. Visualiza√ß√µes explorat√≥rias (boxplot, heatmap)
        5. Determina√ß√£o do n√∫mero √≥timo de clusters (4 m√©tricas)
        6. Aplica√ß√£o do algoritmo K-means
        7. Visualiza√ß√£o dos clusters resultantes
        8. Export dos resultados (consolidado + individual)
        
        Outputs gerados:
        - outputs/figuras/: Todas as visualiza√ß√µes HTML interativas
        - outputs/data/dados_com_clusters.xlsx: Dataset completo com clusters
        - outputs/data/cluster_N.xlsx: Cada cluster em arquivo separado
        
        Returns:
            dict: Dicion√°rio com resultados principais:
                - best_k (int): N√∫mero √≥timo de clusters
                - silhouette_score (float): Score silhouette do resultado
                - arquivo_saida (str): Caminho do arquivo principal
                - n_registros (int): Total de registros processados
                
        Raises:
            FileNotFoundError: Se arquivo de dados n√£o for encontrado
            ValueError: Se dados estiverem em formato inv√°lido
            Exception: Para outros erros durante processamento
        """
        print("üöÄ Iniciando Pipeline de An√°lise de Clustering")
        print("=" * 50)
        
        # 1. Carregar dados
        self.carregar_dados()
        
        # 2. Pr√©-processamento
        self.preprocessar_dados()
        
        # 3. Estat√≠sticas descritivas
        stats, correlacoes = self.estatisticas_descritivas()
        
        # 4. Visualiza√ß√µes explorat√≥rias
        self.criar_visualizacoes_exploratorias()
        
        # 5. Encontrar n√∫mero √≥timo de clusters
        self.encontrar_numero_otimo_clusters()
        
        # 6. Aplicar clustering
        self.aplicar_clustering()
        
        # 7. Visualizar clusters
        self.visualizar_clusters()
        
        # 8. Exportar resultados
        arquivo_saida = self.exportar_dados_com_clusters()
        
        print("\n" + "=" * 50)
        print("‚úÖ Pipeline conclu√≠do com sucesso!")
        print(f"üìÅ Figuras salvas em: outputs/figuras/")
        print(f"üìä Dados exportados: {arquivo_saida}")
        print("=" * 50)
        
        return {
            'best_k': self.best_k,
            'silhouette_score': silhouette_score(self.df_scaled, self.cluster_labels),
            'arquivo_saida': arquivo_saida,
            'n_registros': len(self.df_original)
        }


def main():
    """
    Fun√ß√£o principal para execu√ß√£o do pipeline de clustering.
    
    Esta fun√ß√£o:
    1. Instancia a classe AnaliseClusteringPipeline
    2. Executa todo o pipeline automaticamente
    3. Retorna os resultados principais
    4. Trata exce√ß√µes de forma amig√°vel
    
    Returns:
        dict: Resultados da an√°lise ou None em caso de erro
        
    Example:
        >>> resultados = main()
        >>> print(f"Clusters encontrados: {resultados['best_k']}")
    """
    try:
        # Instanciar e executar o pipeline
        pipeline = AnaliseClusteringPipeline()
        resultados = pipeline.executar_pipeline_completo()
        
        return resultados
        
    except FileNotFoundError as e:
        print(f"‚ùå Erro: Arquivo n√£o encontrado - {str(e)}")
        return None
    except Exception as e:
        print(f"‚ùå Erro inesperado durante a an√°lise: {str(e)}")
        print("Verifique os dados e tente novamente.")
        return None


if __name__ == "__main__":
    # Configurar para ativar ambiente virtual automaticamente
    import subprocess
    import sys
    
    # Verificar se pandas est√° dispon√≠vel
    try:
        import pandas as pd
        print("‚úÖ Ambiente virtual detectado - executando an√°lise...")
        main()
    except ImportError:
        print("‚ùå Pandas n√£o encontrado. Ative o ambiente virtual primeiro:")
        print("source /Volumes/Mac_XIV/virtualenvs/testa_bugs/bin/activate")
        sys.exit(1)
